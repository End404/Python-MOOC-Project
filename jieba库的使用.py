
#*****************************************************************************************#
''' 
--------------------------------- jieba库的使用 -------------------------------------

jieba库:
- 是优秀的中文分词第三方库。
- 中文文本需要通过分词获得单个的词语 。
- jieba是优秀的中文分词第三方库，需要额外安装。
- jieba库提供三种分词模式，最简单只需掌握一个函数。
--------------------------------------------------

jieba分词的原理:
jieba分词依靠中文词库。
- 利用一个中文词库，确定中文字符之间的关联概率。 
- 中文字符间概率大的组成词组，形成分词结果。 
- 除了分词，用户还可以添加自定义的词组。
--------------------------------------------------

jieba分词的三种模式； 
精确模式、全模式、搜索引擎模式。
- 精确模式：把文本精确的切分开，不存在冗余单词。 
- 全模式：把文本中所有可能的词语都扫描出来，有冗余 。
- 搜索引擎模式：在精确模式基础上，对长词再次切分。
--------------------------------------------------

jieba库常用函数：
jieba.lcut(s)   精确模式，返回一个列表类型（s）的分词结果。
jieba.lcut(s, cut_all=True)     全模式，返回一个列表类型的分词结果，存在冗余。
jieba.lcut_for_search(s)    搜索引擎模式，返回一个列表类型的分词（长分词）结果，存在冗余。
jieba.add_word(w)   向分词词典增加新词w。
------------------------------------------------------------------------------------

'''
#*****************************************************************************************#


import jieba
a=jieba.lcut("中国是一个伟大的国家")
print ('精准模式：', a)

print()

print ( '全模式：', jieba.lcut("中国是一个伟大的国家，希望国泰民安、繁荣富强、完成复兴梦。",cut_all=True) )

print()

print ("搜索引擎模式：", jieba.lcut_for_search("中华人民共和国是一个伟大的国家"))

print()

print("增加新词：", a, jieba.add_word( '好很好'))

print()
